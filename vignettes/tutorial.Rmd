---
title: "Project 3: state302util Tutorial"
author: "Junxi Liao"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{state302util Tutorial}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

# Introductory
The package \code{stat302util} contains the functions that are taught and built in the STAT302 course. The functions contained make inferences and/or predictions based on the input data. Namely, this package has the ability to perform a t-test, to fit linear models, to find the k-nearest neighbor cross-validatory classification, and to make random forest cross-validation prediction. 

This package is built following Bryan D Martin's instruction from \link[https://bryandmartin.github.io/STAT302/docs/LectureSlides/lectureslides9/lectureslides9.html#1]. 


Install \texttt{stat302util} using:
```{r, eval = FALSE}
devtools::install_github("hugoliao330/stat302util", build_vignette = TRUE, build_opts = c())
```

To begin, we load our package from library:
```{r, message = FALSE}
library(stat302util)
```

To view the vignette in the stat302util HTML help:
```{r}
help(package = "stat302util", help_type = "html")
```

To view the vignette as an isolated HTML file:
```{r}
utils::browseVignettes(package = "stat302util")
```

# Tutorial for my_t.test
This function gives statistics of the t-score, degree of freedom, and the p-value. 

To start, we will use the \code{lifeExp} data from \code{my_gapminder}. 
```{r}
mgm <- my_gapminder
le <- mgm$lifeExp
```

To test the hypothesis on the \code{lifeExp} data where
\begin{align}
  H_0: \mu &= 60,\\
  H_a: \mu &\neq 60.
  \end{align}

```{r}
my_t.test(x = le, alternative = "two.sided", mu = 60)
``` 
With an alpha threshold of 0.05, this means that we should not rejected the null hypothesis as the p-value exceeds the threshold, and the null hypothesis holds.

To test the hypothesis where 
\begin{align}
  H_0: \mu &= 60,\\
  H_a: \mu &< 60.
  \end{align}

```{r}
my_t.test(x = le, alternative = "less", mu = 60)
```
With $\alpha = 0.05$, we reject the null hypothesis, meaning that the true mean of life expectancy of the population should be less than 60 years old.

To test the hypothesis where
\begin{align}
  H_0: \mu &= 60,\\
  H_a: \mu &> 60.
  \end{align}
```{r}
my_t.test(x = le, alternative = "greater", mu = 60)
```
In this case, we definitely cannot reject the null hypothesis, as this is the exact opposite of the last hypothesis we tested. 

For more information, you can see the documentation of this function by typing this in your console:
```{r}
?my_t.test
```

#Tutorial for my_lm
This function fits the data with a linear regression model on response variable and explanatory variables.

I will demonstrate this function with data from \code{my_gapminder}, where \code{lifeExp} would be my response variable and \code{gdpPercap} and \code{continent} as explanatory variables.

```{r}
ml <- my_lm(formula = lifeExp ~ gdpPercap + continent, mgm)
ml
```

The coefficient for \code{gdpPercap} is `r ml$Estimate[2]`, which is fairly low compared to the coefficients for \code{continent}. This number indicates the correlation between \code{gdpPercap} and the response variable \code{lifeExp}. 

We can propose a hypothesis test associated with the \code{gdpPercap} coefficient, where the null hypothesis is that the coefficient of co-variate \code{gdpPercap} is zero, meaning there is no correlation at all, whereas the alterantive hypothesis would be that the coefficient is not zero, meaning there is a correlation. 

With the information given by the result of \code{my_lm}, and given a p-value cut-off of $\alpha = 0.05$, we can safely conclude that the null hypothesis is rejected and that the alternative hypothesis holds because of the low p-value of `r ml$Pr...t..[2]`. 

We can use a ggplot 2 to visualize the actual verses fitted values.
```{r}
mod_fits <- fitted(my_lm)
my_df <- data.frame(actual = mtcars$mpg, fitted = mod_fits)
ggplot(my_df, aes(x = fitted, y = actual)) +
  geom_point() +
  geom_abline(slope = 1, intercept = 0, col = "red", lty = 2) + 
  theme_bw(base_size = 15) +
  labs(x = "Fitted values", y = "Actual values", title = "Actual vs. Fitted") +
  theme(plot.title = element_text(hjust = 0.5))
```

# Tutorial for my_knn_cv

Again, I will demonstrate this function using data from \code{my_gapminder}.

We can predict output class \code{continent} using covariates \code{gdpPercap} and \code{lifeExp} with a 5-fold cross validation. I will also iterate from 1 to 10 for the kth-nearest-neighbor for this function, in an effort to compare the training misclassification rate as well as the CV misclassification rate.
```{r, warning = FALSE}
train <- data.frame(mgm$gdpPercap, mgm$lifeExp)
true_cl <- mgm$continent
train_mc <- rep(NA, 10)
cv_mc <- rep(NA, 10)

for (i in 1:10) {
  result <- my_knn_cv(train = train, cl = true_cl, k_nn = i, k_cv = 5)
  train_mc[i] <- result$cv_err
  cv_mc[i] <- mean(true_cl != result$class)
}

# show results
train_mc
cv_mc
```

Based on training misclassification rate I would choose \code{k_nn = 9}. Based on the CV misclassification rate I would choose \code{k_nn = 1}. However, choosing the optimal \code{k_nn} value based on true misclassificaiton rates in such a small sample is not reasonable. In practice, we should choose based on the training misclassification rate because it better represents how my model will react to data excluded by the training data. Cross-validation is able to acheive this because it randomly splits the training data in \code{k_cv} folds. Everytime, it uses all but one fold of data to train the model and then to fit it to the last fold of data. This increases flexibility of the model and thus accuracy in unknowns.

# Tutorial for my_rf_cv
As usual, I will use the data from \code{my_gapminder} to demonstrate the functionalities. 


